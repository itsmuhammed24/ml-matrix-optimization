# üîß Optimisation en Apprentissage Automatique (M2 IASD)

Ce projet explore plusieurs techniques d‚Äôoptimisation fondamentales pour l‚Äôapprentissage automatique, √† travers la r√©gression lin√©aire et la factorisation matricielle (rang 1, rang faible, r√©gularis√©e). L‚Äôobjectif est d‚Äôanalyser les effets de la surparam√©trisation, de la r√©gularisation et des m√©thodes de descente (batch vs stochastique) sur la qualit√© des solutions obtenues.

> Projet r√©alis√© dans le cadre du Master IASD , encadr√© par [Cl√©ment Royer](https://www.lamsade.dauphine.fr/~croyer/).

## Contenu

- **R√©gression lin√©aire :**
  - Formulation classique vs surparam√©tr√©e (scalaire vs matrice).
  - Comparaison entre solution analytique et optimisation par descente de gradient.

- **Factorisation matricielle :**
  - Approximation de rang 1 via descente de gradient (GD) et descente stochastique (SGD).
  - √âtude de l‚Äôeffet de la taille de batch sur la convergence.

- **Factorisation matricielle de rang \( r \) :**
  - Impl√©mentation de la factorisation \( UV^\top \) avec rang variable.
  - Comparaison GD/SGD sur des matrices de rang contr√¥l√©.

- **Factorisation r√©gularis√©e :**
  - Ajout de p√©nalit√©s \( L_2 \) sur les matrices \( U \) et \( V \).
  - √âtude de l‚Äôimpact du param√®tre de r√©gularisation \( \lambda \) sur le rang effectif.


